{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DDWfEaegLpTY",
        "JJXhWvZSLvow",
        "EHQfQqGAL3c1"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The examples and data used in this notebook are taken from:\n",
        "https://github.com/materialsvirtuallab/nano281\n"
      ],
      "metadata": {
        "id": "qcLt-cWIw5eU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Roadmap"
      ],
      "metadata": {
        "id": "JbMDf29UnGW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The process of training a machine learning model involves several key steps:\n",
        "\n",
        "1. **Data collection and preparation**: This involves gathering and organizing the data that will be used to train the model. This includes tasks such as cleaning the data, removing irrelevant features, handling missing values, and converting the data into a suitable format for the machine learning algorithm.\n",
        "\n",
        "2. **Feature engineering**: This involves selecting and transforming the input variables (features) that will be used to predict the output variable. This can include tasks such as scaling or normalizing the data, creating new features, and selecting the most relevant features based on domain knowledge or statistical analysis.\n",
        "\n",
        "3. **Model selection**: This involves selecting the appropriate machine learning algorithm for the specific problem and data at hand. This can involve experimenting with different algorithms and hyperparameters, or using pre-trained models for certain types of problems.\n",
        "\n",
        "4. **Model training**: This involves using the selected algorithm and data to train the model by adjusting the model's parameters to minimize the error between the predicted output and the actual output. The training process involves iterating over the data multiple times, adjusting the model's parameters with each iteration to improve its accuracy.\n",
        "\n",
        "5. **Model evaluation**: This involves assessing the performance of the trained model on a separate test dataset, to determine its accuracy and generalization ability. This can involve metrics such as accuracy, precision, recall, F1-score, or mean squared error, depending on the specific problem and type of model.\n",
        "\n",
        "6. **Model deployment**: Once the model has been trained and evaluated, it can be deployed into a production environment for use in real-world applications. This can involve integrating the model into a larger system, building a user interface or API for interacting with the model, and monitoring its performance over time.\n",
        "\n",
        "Overall, the process of training a machine learning model is iterative and often requires careful experimentation and testing to achieve optimal results. It involves a combination of domain knowledge, data analysis, and statistical techniques to build accurate and effective models for a wide range of applications.\n",
        "\n",
        "It is common AI/ML knowledge that step 1 is the most important and time-consuming step (average ~80% of the total time doing AI/ML). Without a cleaned and reliable dataset, no amount of machine learning will be able to get accurate data. "
      ],
      "metadata": {
        "id": "aLkRNaoanXCJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation & Feature Engineering\n",
        "\n"
      ],
      "metadata": {
        "id": "DDWfEaegLpTY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preparation:** Import data from the nano281 github page and filter the data to only keep what we want\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_8KeMr5_nfFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting the data\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/materialsvirtuallab/nano281/master/labs/lab2/data2022.csv\"\n",
        "data = pd.read_csv(url, index_col=0, na_filter=False)\n",
        "df = pd.DataFrame(data)\n",
        "display(df.head())\n",
        "\n",
        "print(\"There are \" + '{:.1f}'.format(len(df)) + \" materials in the data2022.csv.\")"
      ],
      "metadata": {
        "id": "3X3pswAcsdbu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "ac103f73-4e8b-4df2-f1bd-8558f7d0928f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        task_id formula  formation_energy_per_atom  e_above_hull  band_gap  \\\n",
              "297   mp-570140    AuBr                  -0.125181      0.011579    1.9716   \n",
              "573  mp-1206190    ZnI6                   0.387945      0.652470    0.1076   \n",
              "579  mp-1208424   TeBr4                  -0.390060      0.000000    2.5202   \n",
              "738   mp-570480   TcBr4                  -0.404404      0.000000    0.6025   \n",
              "880  mp-1064459     PBr                   1.067789      1.327882    0.0000   \n",
              "\n",
              "     has_bandstructure  \n",
              "297               True  \n",
              "573              False  \n",
              "579              False  \n",
              "738               True  \n",
              "880              False  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-845594a8-b89d-455d-96ee-6b1eaff75da6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>task_id</th>\n",
              "      <th>formula</th>\n",
              "      <th>formation_energy_per_atom</th>\n",
              "      <th>e_above_hull</th>\n",
              "      <th>band_gap</th>\n",
              "      <th>has_bandstructure</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>mp-570140</td>\n",
              "      <td>AuBr</td>\n",
              "      <td>-0.125181</td>\n",
              "      <td>0.011579</td>\n",
              "      <td>1.9716</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>573</th>\n",
              "      <td>mp-1206190</td>\n",
              "      <td>ZnI6</td>\n",
              "      <td>0.387945</td>\n",
              "      <td>0.652470</td>\n",
              "      <td>0.1076</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>579</th>\n",
              "      <td>mp-1208424</td>\n",
              "      <td>TeBr4</td>\n",
              "      <td>-0.390060</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.5202</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>738</th>\n",
              "      <td>mp-570480</td>\n",
              "      <td>TcBr4</td>\n",
              "      <td>-0.404404</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.6025</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>880</th>\n",
              "      <td>mp-1064459</td>\n",
              "      <td>PBr</td>\n",
              "      <td>1.067789</td>\n",
              "      <td>1.327882</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-845594a8-b89d-455d-96ee-6b1eaff75da6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-845594a8-b89d-455d-96ee-6b1eaff75da6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-845594a8-b89d-455d-96ee-6b1eaff75da6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 681.0 materials in the data2022.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Filtering the data\n",
        "\n",
        "temp = df.sort_values(by=['formation_energy_per_atom'], ascending=True) #rearrange by most negative formation energy first for every formula\n",
        "df_lowest = temp.drop_duplicates(subset='formula', keep='first') #delete all duplicates but most negative formation energy per formula\n",
        "\n",
        "df_neg = df_lowest[(df_lowest['formation_energy_per_atom'] <= 0)] #keeping only the formulae with negative formation energies"
      ],
      "metadata": {
        "id": "DzPURgPasgIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Engineering:** is the process of creating new features or transforming existing features in order to improve the performance of a machine learning model. One common technique for feature engineering is to use a design matrix, which is a matrix of predictor variables used in statistical modeling, particularly in regression analysis.\n",
        "\n",
        "By manipulating the design matrix, we can create new features or transform existing ones to better capture the underlying patterns and relationships in the data. \n",
        "\n",
        "However, it is important to be careful when manipulating the design matrix, as it can also introduce issues such as multicollinearity, which can cause instability in the model and make it more difficult to interpret the results. Therefore, it is important to carefully consider the implications of any feature engineering decisions and to evaluate their impact on the model performance.\n",
        "\n",
        "For this example, we will create the `average`, `min`, and `max` features for each of the row of data we have for each material. Open the data import link on your browser and check the data for yourself to understand better what we are trying to do. "
      ],
      "metadata": {
        "id": "qM_k92Xkn9-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Making the Design Matrix\n",
        "# A design matrix is a matrix containing data about multiple characteristics of several individuals or objects. \n",
        "# Each row corresponds to an individual and each column to a characteristic.\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/materialsvirtuallab/nano281/master/labs/lab2/element_properties.csv'\n",
        "data = pd.read_csv(url, index_col=0)\n",
        "element_data = pd.DataFrame(data)\n",
        "\n",
        "EP_mean = element_data.mean(skipna = True) #calculate the mean for each column ignoring all NaN\n",
        "element_datanew = element_data.fillna(EP_mean) #fill in the NaN indexes with the average from their specific column\n",
        "\n",
        "#!pip install pymatgen git+https://github.com/materialsproject/api.git@py37\n",
        "#!pip install mp-api \n",
        "from pymatgen.core import Composition\n",
        "import numpy as np\n",
        "\n",
        "element_datanew[\"composition\"] = [Composition(i).to_data_dict[\"unit_cell_composition\"] for i in element_datanew[\"formula_pretty\"]]\n",
        "\n",
        "def composition_to_dict(c):\n",
        "  if isinstance(c, dict):\n",
        "    unit_cell_composition = c\n",
        "  else:\n",
        "    if isinstance(c, str):\n",
        "      c = Composition(c)\n",
        "      unit_cell_composition = c.to_data_dict[\"unit_cell_composition\"]\n",
        "  return unit_cell_composition\n",
        "\n",
        "def compute_average_from_composition(c, prop):\n",
        "  unit_cell_composition = composition_to_dict(c)\n",
        "  res = 0\n",
        "  total = 0\n",
        "  for i, j in unit_cell_composition.items():\n",
        "    res += element_data.loc[i, prop] * j\n",
        "    total += j\n",
        "  return res / total\n",
        "\n",
        "def get_maxmin_properties(c, prop, mode=\"max\"):\n",
        "  if mode == \"max\":\n",
        "    func = np.max\n",
        "  elif mode == \"min\":\n",
        "    func = np.min\n",
        "  unit_cell_composition = composition_to_dict(c)\n",
        "  res = func([element_data.loc[i, prop] for i in unit_cell_composition])\n",
        "  return res\n",
        "\n",
        "properties = element_data.columns\n",
        "average_properties = []\n",
        "max_properties = []\n",
        "min_properties = []\n",
        "for prop in properties:\n",
        "  average_properties.append([compute_average_from_composition(i, prop) for i in element_datanew[\"composition\"]])\n",
        "  max_properties.append([get_maxmin_properties(i, prop, mode=\"max\") for i in element_datanew[\"composition\"]])\n",
        "  min_properties.append([get_maxmin_properties(i, prop, mode=\"min\") for i in element_datanew[\"composition\"]])\n",
        "\n",
        "average_properties = np.array(average_properties).T\n",
        "max_properties = np.array(max_properties).T\n",
        "min_properties = np.array(min_properties).T\n",
        "\n",
        "average_properties = pd.DataFrame() \n",
        "min_properties = pd.DataFrame()\n",
        "max_properties = pd.DataFrame()\n",
        "\n",
        "design_matrix = np.concatenate([average_properties, max_properties, min_properties], axis=1) #combining the average, min, and max propeties \n",
        "\n",
        "column_names = ([f\"Average {n}\" for n in properties] + [f\"Max {n}\" for n in properties] + [f\"Min {n}\" for n in properties])\n",
        "\n",
        "design_matrix = pd.DataFrame(design_matrix, columns=column_names)"
      ],
      "metadata": {
        "id": "QrJ8lkQrs_fO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "971def71-19fb-4ba5-dc0a-31014905fec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-4c30dabfc13b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#!pip install pymatgen git+https://github.com/materialsproject/api.git@py37\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#!pip install mp-api\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpymatgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mComposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pymatgen'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature scaling** is an important step in feature engineering that involves transforming the numerical features in a dataset to a common scale. This can be necessary because many machine learning algorithms work better when the features are on a similar scale, and some algorithms can even fail to converge or produce biased results if the features are not scaled properly.\n",
        "\n",
        "Scaling the features is typically done after creating the design matrix, as it involves transforming the numerical columns of the design matrix. The scaled features can then be included in the design matrix as predictor variables for machine learning algorithms."
      ],
      "metadata": {
        "id": "-vLDzSLgraRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Scaling\n",
        "\n",
        "#X is your materials data \n",
        "#Y is your design matrix \n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#Method 1 part a\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(x)\n",
        "means_ = scaler.mean_\n",
        "stds_ = scaler.scale_\n",
        "z = scaler.transform(x)\n",
        "\n",
        "#Method 1 part b\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(z)\n",
        "z_pca = pca.transform(z)\n",
        "\n",
        "#Method 2\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(X) \n",
        "\n",
        "#Method 3\n",
        "norm_train_X = (train_X - train_X.mean())/train_X.std()\n",
        "norm_test_X = (test_X - train_X.mean())/train_X.std()\n",
        "\n",
        "for column in train_X.columns: \n",
        "    norm_train_X[column] = (train_X[column] - \\\n",
        "        train_X[column].mean()) / train_X[column].std()     \n",
        "\n",
        "for column in test_X.columns: \n",
        "    norm_test_X[column] = (test_X[column] - \\\n",
        "        test_X[column].mean()) / test_X[column].std() \n"
      ],
      "metadata": {
        "id": "5Ezm7n2Flkwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Linear Regression"
      ],
      "metadata": {
        "id": "JJXhWvZSLvow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regression** is a type of statistical analysis used to model the relationship between a dependent variable and one or more independent variables.\n",
        "\n",
        "**Linear Regression:** This is the simplest and most widely used type of regression. It assumes a linear relationship between the dependent variable and the independent variables. Linear regression can be either simple (one independent variable) or multiple (more than one independent variable).\n",
        "\n",
        "**How does k folds work?** High k => more computation time largely irrelevant, as you anyways want to calculate many models.\n",
        "Usually, large k mean less (pessimistic) bias. If possible, I use a k that is a divisor of the sample size, or the size of the groups in the sample that should be stratified."
      ],
      "metadata": {
        "id": "DHbje1k0r1f1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "HTSZ_6kALmaH",
        "outputId": "82e76390-35f0-47c4-e858-386b31d91a06"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-f62c2bdeba2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0myhat_mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Assume you did test split, scaled, and what not already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mr2_mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat_mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmse_mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat_mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'z' is not defined"
          ]
        }
      ],
      "source": [
        "#Linear Regression\n",
        "from sklearn import linear_model \n",
        "from sklearn.model_selection import cross_val_predict, KFold\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42) #42 because it is not too big or small.\n",
        "\n",
        "mlr = linear_model.LinearRegression()\n",
        "yhat_mlr = cross_val_predict(mlr, z, y, cv=kfold) #Assume you did test split, scaled, and what not already.\n",
        "r2_mlr = r2_score(y, yhat_mlr)\n",
        "mse_mlr = mean_squared_error(y, yhat_mlr)\n",
        "label_mlr = \"MLR: $R^2$ = %.3f, MSE = %.1f\" % (r2_mlr, mse_mlr)\n",
        "\n",
        "f, ax = plt.subplots(figsize=(8, 8))\n",
        "plt.plot(y, yhat_mlr, \"o\", label=label_mlr)\n",
        "plt.ylabel(r\"$K_{predicted}$ (GPa)\")\n",
        "plt.xlabel(r\"$K$ (GPa)\")\n",
        "plt.legend()\n",
        "plt.xlim([0, 410])\n",
        "plt.ylim([0, 410])\n",
        "plt.plot([0, 410], [0, 410], \"k--\");"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ridge Regression:** This is a type of regression that is used to overcome the problem of multicollinearity (high correlation) among the independent variables. Ridge regression adds a penalty term to the least squares objective function to control the size of the coefficients.\n",
        "\n",
        "**GridSearchCV (Grid Search Cross-Validation)** is a technique in machine learning used for hyperparameter tuning, which refers to the process of selecting the best combination of hyperparameters for a machine learning model. It automates the process of trying out different combinations of hyperparameters for a model by creating a \"grid\" of all possible hyperparameter values to try. It then trains the model using each combination of hyperparameters and evaluates its performance using cross-validation."
      ],
      "metadata": {
        "id": "7LAFenMTsXJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ridge\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "alphas = np.linspace(0.0001, 0.005, 100)\n",
        "model = Ridge(max_iter=100000)\n",
        "gcv = GridSearchCV(model, param_grid={\"alpha\": alphas}, scoring=\"neg_mean_squared_error\", cv=kfold)\n",
        "\n",
        "gcv.fit(z, y)\n",
        "alphas = gcv.cv_results_[\"param_alpha\"]\n",
        "\n",
        "alpha_optimal = gcv.best_params_[\"alpha\"]\n",
        "ridge_best = gcv.best_estimator_\n",
        "\n",
        "pred_test_ridge = ridge_best.predict(test_scaled)"
      ],
      "metadata": {
        "id": "KtyO5NHbL1Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This next section repeats the above code but define a function to perform the GridSearchCV hyperparameter tuning. "
      ],
      "metadata": {
        "id": "YFiW51zSsnqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#More accurate best alpha\n",
        "ridge = linear_model.Ridge(max_iter=100000)\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV \n",
        "import seaborn as sns\n",
        "#Function imported from lecture to plot Gridsearch CV to visualize parameters\n",
        "def plot_grid_search_results(gs, ylim=None):\n",
        "    \"\"\"\n",
        "    Plots the results of GridSearchCV.\n",
        "\n",
        "    Args:\n",
        "        gs: A GridSearchCV object.\n",
        "        ylim: Optional setting for y limits.\n",
        "    \"\"\"\n",
        "    results = pd.DataFrame(gs.cv_results_)\n",
        "    for c in results.columns:\n",
        "        # Note that here we are working with just variations in one parameter.\n",
        "        # So we can automatically find the name of that parameter.\n",
        "        if c.startswith(\"param_\"):\n",
        "            x = c\n",
        "            break\n",
        "    fig, ax = plt.subplots(figsize=(16, 8))\n",
        "    ax = sns.lineplot(x=x, y=\"mean_train_score\", data=results)\n",
        "    ax = sns.scatterplot(x=x, y=\"mean_train_score\", data=results, marker=\"x\")\n",
        "    ax = sns.lineplot(x=x, y=\"mean_test_score\", data=results)\n",
        "    ax = sns.scatterplot(x=x, y=\"mean_test_score\", data=results, marker=\"o\")\n",
        "    plt.xlabel(x)\n",
        "    if ylim:\n",
        "        plt.ylim(ylim)\n",
        "    ax.legend([\"Train\", \"Test\"], loc=2)\n",
        "#This uses gridsearch to find optimal parameters based on the best MAE\n",
        "def find_best_params(x, y, model, params): \n",
        "    gs = GridSearchCV(\n",
        "        model,\n",
        "\n",
        "        param_grid=params,\n",
        "        return_train_score=True,\n",
        "        scoring=\"neg_mean_absolute_error\",\n",
        "        cv=kfold,\n",
        "        n_jobs= coresToUse, )\n",
        "    gs.fit(x,y)\n",
        "    results = pd.DataFrame(gs.cv_results_)\n",
        "    display(\"mean_test_score\")\n",
        "    print(\"The range MAE values is \", np.ptp(results['mean_test_score']), \" with a standard deviation of \", np.std(results['mean_test_score']), \" and an average of\", np.average(results['mean_test_score']))\n",
        "    best_result = results[results['rank_test_score']==results['rank_test_score'].min()]\n",
        "    plot_grid_search_results(gs)\n",
        "    return best_result\n",
        "\n",
        "result = find_best_params(x,y,ridge, {\"alpha\": np.logspace(-10, 1, 30)})\n",
        "print(\"For the best params \", dict(result['params']), \" mae score is \", result['mean_test_score'] )"
      ],
      "metadata": {
        "id": "K3VW3gNBq0Sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After obtaining the best parameters in Ridge, you can create a new Linear regression model with your best `alpha` parameter."
      ],
      "metadata": {
        "id": "WihJUZPJtyuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_ridge = linear_model.Ridge(max_iter=100000, alpha= #best alpha)\n",
        "yhat = cross_val_predict(final_ridge, z, y, cv=kfold)\n",
        "mae = mean_absolute_error(y,yhat)\n",
        "print(\"The mean absolute error is\" , mae)"
      ],
      "metadata": {
        "id": "6xSJbQ6kq5bX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lasso Regression:** This is similar to ridge regression but uses a different penalty term to shrink the coefficients of the independent variables towards zero. Lasso regression is useful for variable selection and can be used to identify the most important predictors."
      ],
      "metadata": {
        "id": "fvSVrFDkt9iI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lasso \n",
        "from sklearn import GridSearchCV, Lasso\n",
        "alphas = np.linspace(0.0001, 0.005, 100)\n",
        "model = Lasso(max_iter=100000)\n",
        "gcv = GridSearchCV(model, param_grid={\"alpha\": alphas}, scoring=\"neg_mean_squared_error\", cv=kfold)\n",
        "\n",
        "gcv.fit(z, y)\n",
        "alphas = gcv.cv_results_[\"param_alpha\"]\n",
        "\n",
        "alpha_optimal = gcv.best_params_[\"alpha\"]\n",
        "lasso_best = gcv.best_estimator_\n",
        "\n",
        "pred_test_lasso = lasso_best.predict(test_scaled)"
      ],
      "metadata": {
        "id": "W-Sqxs6MLzs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To learn more about Lasso and Ridge, their differences, and when either shine more than the other, check out this link: https://www.datacamp.com/tutorial/tutorial-lasso-ridge-regression"
      ],
      "metadata": {
        "id": "EueXuEyBrTlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tree-based Regression"
      ],
      "metadata": {
        "id": "EHQfQqGAL3c1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tree-based regression** is a type of supervised learning algorithm that is used to predict a continuous output variable, given one or more input variables. It is a non-parametric method, which means that it makes no assumptions about the distribution of the data. Instead, it learns the relationships between the input variables and the output variable by constructing a tree-like model.\n",
        "\n",
        "This means that often for tree-based regression models, performing feature scaling or normalizing is not necessary, unlike the above linear regression models when data scaling is more important. \n",
        "\n",
        "The tree-based regression algorithm works by recursively splitting the data into subsets, based on the values of the input variables. The splits are chosen to minimize the variance of the output variable within each subset. This means that the algorithm tries to find the values of the input variables that are most strongly associated with the output variable, and then splits the data based on those values."
      ],
      "metadata": {
        "id": "1q0qmb7YugUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Decision Tree\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import export_text\n",
        "\n",
        "decision_treeR = DecisionTreeRegressor(ccp_alpha=0.0001, criterion='mse', random_state=2, max_depth=15, max_features=30, min_samples_leaf=1, min_samples_split=3, spliiter='best')\n",
        "decision_treeR = decision_treeR.fit(norm_train_X, train_y['log10K'])\n",
        "\n",
        "pred_DTR = decision_treeR.predict(norm_test_x) #Predictions on Testing data\n",
        "print(decision_treeR.score(norm_test_X, test_y['log10K']))"
      ],
      "metadata": {
        "id": "Ed7JsJFGL55i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DT Gradiant Boosting\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "import sklearn.metrics\n",
        "\n",
        "def modelfit(alg, dtrain, predictors, performCV=True, printFeatureImportance=True, cv_folds=5):\n",
        "    #Fit the algorithm on the data\n",
        "    alg.fit(scaled_data,y)\n",
        "        \n",
        "    #Predict training set:\n",
        "    dtrain_predictions = alg.predict(scaled_data)\n",
        "    dtrain_predprob = alg.predict_proba(scaled_data)[:,1]\n",
        "    \n",
        "    #Perform cross-validation:\n",
        "    if performCV:\n",
        "        cv_score = cross_val_predict.cross_val_score(alg,scaled_data,y, cv=cv_folds, scoring='roc_auc')\n",
        "    \n",
        "    #Print model report:\n",
        "    print (\"\\nModel Report\")\n",
        "    print (\"Accuracy : %.4g\" % metrics.accuracy_score(y.values, dtrain_predictions))\n",
        "    print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(y, dtrain_predprob))\n",
        "    \n",
        "    if performCV:\n",
        "        print (\"CV Score : Mean - %.7g | Std - %.7g | Min - %.7g | Max - %.7g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n",
        "        \n",
        "    #Print Feature Importance:\n",
        "    if printFeatureImportance:\n",
        "        feat_imp = pd.Series(alg.feature_importances_, predictors).sort_values(ascending=False)\n",
        "        feat_imp.plot(kind='bar', title='Feature Importances')\n",
        "        plt.ylabel('Feature Importance Score')\n",
        "\n",
        "#Choose all predictors except target & IDcols\n",
        "param_test1 = {'n_estimators':range(20,81,10)}\n",
        "gsearch1 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.1, min_samples_split=500,min_samples_leaf=50,max_depth=8,max_features='sqrt',subsample=0.8,random_state=10),\n",
        "                        param_grid = param_test1, scoring='roc_auc',n_jobs=-1, cv=11)\n",
        "gsearch1.fit(scaled_data,y)\n",
        "\n",
        "gsearch1.best_params_"
      ],
      "metadata": {
        "id": "Kp59-Io_ILpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_test2 = {'max_depth':range(5,16,2), 'min_samples_split':range(200,1001,200)}\n",
        "gsearch2 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.1, n_estimators=60, max_features='sqrt', subsample=0.8, random_state=10), \n",
        "                        param_grid = param_test2, scoring='roc_auc',n_jobs=-1, cv=5)\n",
        "gsearch2.fit(scaled_data,y)\n",
        "gsearch2.best_params_"
      ],
      "metadata": {
        "id": "J8vOoA-SNRew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_test3 = {'min_samples_split':range(100,2000,200), 'min_samples_leaf':range(10,71,10)}\n",
        "gsearch3 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.1, n_estimators=60,max_depth=9,max_features='sqrt', subsample=0.8, random_state=10), \n",
        "                                    param_grid = param_test3, scoring='roc_auc',n_jobs=-1, cv=5)\n",
        "gsearch3.fit(scaled_data,y)\n",
        "gsearch3.best_params_"
      ],
      "metadata": {
        "id": "zu0Tq6-yNnTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_test4 = {'max_features':range(2,20,2)}\n",
        "gsearch4 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.1, n_estimators=60,max_depth=9, min_samples_split=1200, min_samples_leaf=60, subsample=0.8, random_state=10),\n",
        "                        param_grid = param_test4, scoring='roc_auc',n_jobs=-1, cv=5)\n",
        "gsearch4.fit(scaled_data,y)\n",
        "gsearch4.best_params_"
      ],
      "metadata": {
        "id": "FNaikwuWNvjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_test5 = {'subsample':[0.6,0.7,0.75,0.8,0.85,0.9]}\n",
        "gsearch5 = GridSearchCV(estimator = GradientBoostingRegressor(learning_rate=0.1, n_estimators=20,max_depth=9,min_samples_split=1200, min_samples_leaf=60, subsample=0.8, random_state=10,max_features=7),\n",
        "param_grid = param_test5, scoring='roc_auc',n_jobs=-1, cv=5)\n",
        "gsearch5.fit(scaled_data,y)\n",
        "gsearch5.best_params_"
      ],
      "metadata": {
        "id": "mKEmjO9ON24t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So I did this in part because google colab can't compute that much. The last time I did this in one go, it took over 4 hours. I simply stop it."
      ],
      "metadata": {
        "id": "QfMcPKwzN9wE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "DTGBR = GradientBoostingRegressor(learning_rate=0.01, n_estimators=100, max_depth=9, min_samples_split=100, min_samples_leaf=10, subsample=0.8, random_state=42)\n",
        "DTGBR = DTGBR.fit(scaled_data, y)\n",
        "\n",
        "pred_DTGBR = DTGBR.predict(test_scaled)\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "rmse_test = MSE(sample_file['log10K_predicted'], df_DTGBR['log10K_predicted'])**(1/2)\n",
        "print(rmse_test)"
      ],
      "metadata": {
        "id": "GR78deCoN5g7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Other Tree ML alogrithms: **\n",
        "\n",
        "**DT Adaboost** -> https://towardsdatascience.com/understanding-adaboost-for-decision-tree-ff8f07d2851\n",
        "\n",
        "**Extra Tree** is an ensemble machine learning algorithm that combines the predictions from many decision trees. -> https://machinelearningmastery.com/extra-trees-ensemble-with-python/\n",
        "\n",
        "**Random Forest** Decision trees are also computationally expensive to train, carry a big risk of overfitting and tend to find local optima because they can’t go back after they have made a split. To address these weaknesses, we turn to random forest, which illustrates the power of combining many decision trees into one model. -> https://builtin.com/data-science/random-forest-python\n",
        "\n",
        "**XGBoost** is an implementation of gradient boosted decision trees designed for speed and performance that is dominative competitive machine learning. -> https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/ \n",
        "\n",
        "In essence Decision Tree < Gradient = Adaboost < Extra Tree < Random Forest < XGBoost"
      ],
      "metadata": {
        "id": "EKq0F-PgOQC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network"
      ],
      "metadata": {
        "id": "tnXKnaQVQ20v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A neural network** is a type of machine learning algorithm that is inspired by the structure and function of the human brain. It consists of multiple interconnected nodes, or neurons, that work together to process and learn from data. It is often refers to as the \"newest and greatest\" for AI/ML applications.\n",
        "\n",
        "Neural networks have been used to solve a wide range of machine learning problems, including image classification, speech recognition, natural language processing, and game playing. They are particularly well-suited for problems with large amounts of data and complex patterns, where traditional machine learning algorithms may struggle to find meaningful relationships.\n",
        "\n",
        "However, neural networks can be difficult to train and optimize, and may require large amounts of computational resources to achieve good performance. They also tend to be \"black box\" models, meaning that it can be difficult to interpret how they are making their predictions. As such, they require careful tuning and evaluation to ensure that they are accurately capturing the underlying relationships in the data."
      ],
      "metadata": {
        "id": "6IkgWpYiwOgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Most Basic NN\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "nn = MLPRegressor(hidden_layer_sizes=(5,3), alpha=1e-6, max_iter=5000, learning_rate_init=0.001, activation='relu', batch_size=335) #These numbers will affect it. Find optimal ones\n",
        "nn.fit(scaled_data, y)\n",
        "\n",
        "pred_SKNN = nn.predict(test_scaled)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "rmse_test = MSE(sample_file['log10K_predicted'], df_SKNN['log10K_predicted'])**(1/2)\n",
        "print(rmse_test)"
      ],
      "metadata": {
        "id": "ybxJI8ykQ6-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bayesian NN\n",
        "\n",
        "#First, do the instal below because Google Colab doesn't have the package\n",
        "#!pip install bayesian-optimization \n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, BatchNormalization, Dropout\n",
        "from keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from math import floor\n",
        "from sklearn.metrics import make_scorer, accuracy_score\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from keras.layers import LeakyReLU\n",
        "LeakyReLU = LeakyReLU(alpha=0.1)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "\n",
        "def nn_cl_bo(neurons, activation, optimizer, learning_rate,  batch_size, epochs ):\n",
        "    optimizerL = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','SGD']\n",
        "    optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n",
        "                 'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n",
        "                 'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n",
        "                 'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
        "    activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
        "                   'elu', 'exponential', LeakyReLU,'relu']\n",
        "    neurons = round(neurons)\n",
        "    activation = activationL[round(activation)]\n",
        "    batch_size = round(batch_size)\n",
        "    epochs = round(epochs)\n",
        "    def nn_cl_fun():\n",
        "        opt = Adam(lr = learning_rate)\n",
        "        nn = Sequential()\n",
        "        nn.add(Dense(neurons, input_dim=10, activation=activation))\n",
        "        nn.add(Dense(neurons, activation=activation))\n",
        "        nn.add(Dense(1, activation='sigmoid'))\n",
        "        nn.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "        return nn\n",
        "    es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n",
        "    nn = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size,\n",
        "                         verbose=0)\n",
        "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
        "    score = cross_val_score(nn, scaled_data, trainY, scoring=score_acc, cv=kfold, fit_params={'callbacks':[es]}).mean()\n",
        "    return score\n",
        "\n",
        "# Set paramaters\n",
        "params_nn ={\n",
        "    'neurons': (10, 100),\n",
        "    'activation':(0, 9),\n",
        "    'optimizer':(0,7),\n",
        "    'learning_rate':(0.0001, 1),\n",
        "    'batch_size':(200, 1000),\n",
        "    'epochs':(20, 100),\n",
        "}\n",
        "# Run Bayesian Optimization\n",
        "nn_bo = BayesianOptimization(nn_cl_bo, params_nn, random_state=111)\n",
        "nn_bo.maximize(init_points=25, n_iter=4) \n",
        "\n",
        "#Gets the best parameters\n",
        "params_nn_ = nn_bo.max['params']\n",
        "activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
        "               'elu', 'exponential', LeakyReLU,'relu']\n",
        "params_nn_['activation'] = activationL[round(params_nn_['activation'])]\n",
        "params_nn_\n",
        "\n",
        "#Run like normal"
      ],
      "metadata": {
        "id": "ZfyiM9KcRenr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Other NN ML alogrithms: *\n",
        "\n",
        "**NN Gaussian Process** -> https://towardsdatascience.com/gaussian-process-models-7ebce1feb83d\n",
        "\n",
        "**TensorFlow** allows you to fine tune the hyperparameters much more easily than sklearn and takes less time as well. It is for large datasets and object detection and need excellent functionality and high performance. -> https://www.tensorflow.org/datasets/keras_example \n",
        "\n",
        "**PyTorch** similar to TensorFlow, but is more friendly if coming from basic python. It is used more in the realm of AI.  -> https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html \n",
        "\n",
        "More complex NN requires knowing what **Keras** is. It is used for small datasets, rapid prototyping, and multiple back-end support. More here -> https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/ \n",
        "\n",
        "The differences between TensorFlow, PyTorch, and Keras -> https://www.simplilearn.com/keras-vs-tensorflow-vs-pytorch-article \n",
        "\n",
        "In terms of user friendliness: sklearn > PyTorch > Keras > TensorFlow.\n",
        "\n",
        "In terms of speed: TensorFlow > PyTorch > Keras > sklearn\n",
        "\n",
        "Research: PyTorch\n",
        "\n",
        "Industry: TensorFlow and Keras"
      ],
      "metadata": {
        "id": "jOWtXZeESSEg"
      }
    }
  ]
}